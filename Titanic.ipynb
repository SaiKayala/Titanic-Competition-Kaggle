{"cells":[{"metadata":{"_uuid":"3878aa15872d70149c87463d483f66f48f3ab7e6"},"cell_type":"markdown","source":"# Titanic: Machine Learning from Disaster"},{"metadata":{"_uuid":"ab3c363a552c290812a34e85acddfd48935e6fba"},"cell_type":"markdown","source":"* __Team Name:__ UMN-STAT-5302\n* __Team Members :__ Anick Saha, Gerrit Vreeman, Karthik Unnikrishnan, Manish Rai, Sai Kumar Kayala. "},{"metadata":{"_uuid":"63dea15078d00cbb9c249c3ab30a10ac95a7734f"},"cell_type":"markdown","source":"## Abstract\n\n### What is that we're trying to solve?\n\nThe sinking of the Titanic is one of the most infamous shipwrecks in history. On April 15, 1912, during her maiden voyage, the Titanic sank after colliding with an iceberg, killing 1502 out of 2224 passengers and crew. \n\nOne of the reasons that the shipwreck led to such loss of life was that there were not enough lifeboats for the passengers and crew. Although there was some element of luck involved in surviving the sinking, _some groups of people were more likely to survive than others, such as women, children, and the upper-class._\n\nIn this task, we complete the analysis of what sorts of people were likely to survive by applying the tools of machine learning to predict which passengers survived the tragedy.\n\n### How did we approach the problem? \n\nThere were the main stages in how we solved this problem:\n\n1. Data Summarization. \n2. Data Preprocessing.\n3. Feature Engineering.\n4. Try out algorithms that we felt would be a good fit to this problem. \n5. Compare and try to improve accuracies by repeating Step 3 and Step 4 to find the model with the best accuracy.\n\nBased on our analysis, the best accuracy was achieved by using the Random Forest Classifier.\n\n### Data Summary and EDA:\n\n##### Intro:\n\nWe performed extensive data analysis to get a better idea about the data. We looked at the summaries of the data at various levels like - Class level, Sex level, Class-Sex level, and the NamePrefix (which is a derived variable from the Name) level. \n\nSome of the observations that we saw are:\n\n1. Survival in the female is higher than male.\n2. Survival rate for class1 is higher than survival rate in class2 which in turn is higher than the survival rate in class3.\n3. We also notice that in general the age of people in class1 is greater than the age of people in class2 which is greater than the age of people in class3.\n\nWe also looked at interaction of variables for survivals and non-survivals and made some interesting plots in R.\n\n[![text](https://raw.githubusercontent.com/anicksaha/blob/master/stat5302/Titanic_Charts.png)](https://github.com/anicksaha/blob/blob/master/stat5302/Titanic_Charts.png)\n\n##### What preprocessing did we do?\n\n1. We checked for NULL values in each column and noticed that there are a number of null values Cabin, Age and Fare.\n2. In order to imute the null vaules in Fare, we use linear interpolation since the number of null vaules is very small.\n3. For imputing age, we took the mean of the age column. \n4. Due to the large number of null values in the cabin column, we decide to drop it.\n5 We also drop the ticket column because it does not seem to have any relation with survival\n\n##### What sort of Feaure Engineering were done?\n1. We created the feature FamilySize to denote the number of people travelling together by adding SibSp and Parch.\n2. The feature IsAlone was created to denote if a passenger is travelling alone or not\n3. The feature Title is extracted from the Name attribute and the rare titles were mapped to more general ones. After this, the name column is dropped.\n4. For the remaining feateures we performed label encoding for the categoriacal variables\n\n"},{"metadata":{"trusted":false,"_uuid":"7f71392f3a2cfefe0536837666aa9200a4a806aa"},"cell_type":"markdown","source":"### Different models that were tried:\nWe tried the following models with the same proprocessed data:\n1. Logistic regression\n2. SVM\n3. AdaBoost Classifier\n4. XGBoost Classifier\n5. ExtraTrees Classifier\n6. GradientBoosting Classifier\n7. GaussianNB Classifier\n8. Random Forest Classifer\n\n##### How did each model fair?\n* For each of the models, we calculated the accuracy in terms of the training dataset. \n* Amongst all the models, RandomForest classifier and ExtraTrees classifier gave the greterst accuracy valus. \n* Of these two, the Random Forest Classifier performed better on the Test dataset (Kaggle submission score) and hence generalized better!"},{"metadata":{"_uuid":"2ac1a963d7a30a2e21a6d0567ed45e3fecc38d37"},"cell_type":"markdown","source":"### The model that won out heart - Random Forest!\n\nRandom Forest is a supervised learning algorithm. The forest it builds, is an ensemble of Decision Trees, most of the time trained with the “bagging” method. \n\nTo say it in simple words: Random forest builds multiple decision trees and merges the rules together to get a more accurate Decision Tree. \n\nOne of the big problems in machine learning is overfitting, but most of the time this won’t happen that easy to a random forest classifier. That’s because if there are enough trees in the forest, the classifier won’t overfit the model. The complex models we used like SVM don't work well with this data because of the low number of training samples available while a random forest being a very simple yet powerful model did a better job and turned out to be a better fit for the low number of traning samples.\n\n### Conclusion:\n\nFuture work: \n\n* We can try different __imputation methods__ like - Groupwise means, medians, etc.\n* We can try __ensembling different models__ and make a more robust model that generalizes better.\n*  We can try to think of a way to use 'ticket_number' and other dropped columns. \n\n\n\n"},{"metadata":{"_uuid":"dc9a37fffe7d1934e590a457d308b44e31c649c1"},"cell_type":"markdown","source":"# Py Notebook:"},{"metadata":{"trusted":true,"_uuid":"fa36fac1ce15b797ba479c5ec7b8e2836a83bd34"},"cell_type":"code","source":"### Import necessary Libraries and Data\nimport random\nimport numpy as np\nimport pandas as pd\nimport matplotlib.pyplot as plt\nimport seaborn as sns\nfrom sklearn.preprocessing import LabelEncoder,OneHotEncoder\nfrom sklearn import model_selection\nfrom sklearn import ensemble,linear_model,tree,svm,naive_bayes\n\n\n\nsns.set_style('white')\nimport warnings\nwarnings.filterwarnings('ignore')","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"85fad432d8d24e6852786898bea3351f064ffa2d"},"cell_type":"code","source":"data_train=pd.read_csv(\"../input/train.csv\")\ndata_test=pd.read_csv(\"../input/test.csv\")","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"be0cb9e7f63f1645e61363887908476484f28ff6"},"cell_type":"code","source":"data_train.info()\nprint('-'*25)\ndata_test.info()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"77a2300c92bcdcd32f539bcd4132313e47d235ab"},"cell_type":"markdown","source":"Checking for null values"},{"metadata":{"trusted":true,"_uuid":"09f987f066d301a0b96c3ac353f7c71498f01483"},"cell_type":"code","source":"print(data_train.isnull().sum())\nprint('*'*25)\nprint(data_test.isnull().sum())","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"e9e4de63a72bf88b410c79831228a24d50d28689"},"cell_type":"markdown","source":"We have null values in Age,Cabin and Embarked for training data and Age Fare and Cabin have null values in test data.\n"},{"metadata":{"trusted":true,"_uuid":"5fb4f4a0d489e632ea321ce65b2474aef8e6a9bd"},"cell_type":"code","source":"data_cpy=data_train.copy(deep=True)\ndata=[data_cpy,data_test]\nmin_num=5","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"1fc65ab61478639c3a71e189fe3d341ac2f1abb3"},"cell_type":"code","source":"for dataset in data:\n    dataset['Age']=dataset['Age'].fillna(dataset['Age'].mean())\n    dataset['Embarked']=dataset['Embarked'].fillna(dataset['Embarked'].mode()[0])\n    dataset['Fare']=dataset['Fare'].fillna(dataset['Fare'].interpolate(method='linear'))\ndata_cpy['FamilySize']=data_cpy['SibSp']+data_cpy['Parch']+1\ndata_test['FamilySize']=data_test['SibSp']+data_test['Parch']+1\ndata_test['isAlone']=1\ndata_test['isAlone'].loc[data_test['FamilySize']>1]=0\ndata_cpy['isAlone']=1\ndata_cpy['isAlone'].loc[data_cpy['FamilySize']>1]=0\ndata_cpy['Title']=data_cpy['Name'].str.split(', ',expand=True)[1].str.split('.',expand=True)[0]\ndata_test['Title']=data_test['Name'].str.split(', ',expand=True)[1].str.split('.',expand=True)[0]\nTitle_Dictionary = {\n    \"Capt\": \"Officer\",\n    \"Col\": \"Officer\",\n    \"Major\": \"Officer\",\n    \"Jonkheer\": \"Royalty\",\n    \"Don\": \"Royalty\",\n    \"Sir\" : \"Royalty\",\n    \"Dr\": \"Officer\",\n    \"Rev\": \"Officer\",\n    \"the Countess\":\"Royalty\",\n    \"Mme\": \"Mrs\",\n    \"Mlle\": \"Miss\",\n    \"Ms\": \"Mrs\",\n    \"Mr\" : \"Mr\",\n    \"Mrs\" : \"Mrs\",\n    \"Miss\" : \"Miss\",\n    \"Master\" : \"Master\",\n    \"Lady\" : \"Royalty\",\n    \"Dona\" : \"Mrs\"\n}\ndata_cpy['Title']=data_cpy.Title.map(Title_Dictionary)\ndata_test['Title']=data_test.Title.map(Title_Dictionary)\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"3ea0ed8e1e54e4a11635ec730ed2216494083e5c"},"cell_type":"code","source":"drop_col=['Cabin','Name','Ticket']\ndata_cpy.drop(columns=drop_col,axis=1,inplace=True)\ndata_test.drop(columns=drop_col,axis=1,inplace=True)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"bc99ab72ebf113e0ab13ffeb3255e6af126cf269"},"cell_type":"code","source":"data_cpy.head()","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"72dcb3f549fb12cde2cf506e33d7b3f3d997b628"},"cell_type":"markdown","source":"### Converting"},{"metadata":{"trusted":true,"_uuid":"02a0a580411565d6a5cb363f6f634fde34db5848"},"cell_type":"code","source":"lbl=LabelEncoder()\nfor d in data:\n    d['Sex']=lbl.fit_transform(d['Sex'])\n    d['Embarked']=lbl.fit_transform(d['Embarked'])\n    d['Title']=lbl.fit_transform(d['Title'])\n    d['Age']=pd.qcut(d['Age'].astype(int),4)\n    d['Fare']=pd.qcut(d['Fare'].astype(int),4)\n    d['Fare']=lbl.fit_transform(d['Fare'])\n    d['Age']=lbl.fit_transform(d['Age'])\ndata_cpy=pd.get_dummies(data_cpy,columns=['Sex','Embarked','Title','Fare','Age'])\ndata_test=pd.get_dummies(data_test,columns=['Sex','Embarked','Title','Fare','Age'])\n\n","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"f3589e2ad78d8a359ad472da13cab34b70535818"},"cell_type":"code","source":"data_cpy.head()","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"aea9ba95852dbbfa07e813a7b70fdf1651d8af3d"},"cell_type":"code","source":"train_col=['Pclass','Sex_0','Sex_1','Age_0','Age_1','Age_2','Age_3','FamilySize','isAlone','Fare_0','Fare_1','Fare_2','Fare_3','Embarked_0','Embarked_1','Embarked_2','Title_0','Title_1','Title_2','Title_3','Title_4']\ntarget=['Survived']\nX_train=data_cpy[train_col].copy(deep=True)\nY_train=data_cpy[target]\nprint(X_train.dtypes)\nX_train.head()\ndata_test1=data_test[train_col].copy(deep=True)","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"b73ab8a8506e39e7623b67855af672c3d3628af8"},"cell_type":"markdown","source":"### Model"},{"metadata":{"trusted":true,"_uuid":"3e05a24973875b03083c3bd503a6821db93d523a"},"cell_type":"code","source":"parameters = {'bootstrap': False, 'min_samples_leaf': 3, 'n_estimators': 50, \n                  'min_samples_split': 10, 'max_features': 'sqrt', 'max_depth': 6}\nclf=ensemble.ExtraTreesClassifier()\nclf.fit(X_train,Y_train)\ntrain_score=clf.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"6e89a97082d0896d78e78819cbbeae6765deb86b"},"cell_type":"code","source":"from sklearn.ensemble import AdaBoostClassifier\nada=AdaBoostClassifier(n_estimators=200,random_state=0,learning_rate=0.05)\nada.fit(X_train,Y_train)\ntrain_score=ada.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"65b49486b8de9c8695218b278e1aab90c5ae9507"},"cell_type":"code","source":"import xgboost as xg\nxgboost=xg.XGBClassifier(n_estimators=900,learning_rate=0.1)\nxgboost.fit(X_train,Y_train)\ntrain_score=xgboost.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4bdd68eaf6b36bc686e06d5acf7705e9d57dbd44"},"cell_type":"code","source":"from sklearn.ensemble import GradientBoostingClassifier\ngrad=GradientBoostingClassifier(n_estimators=500,random_state=0,learning_rate=0.1)\ngrad.fit(X_train,Y_train)\ntrain_score=grad.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"7fd78c074535e1d77c6c160838d3ce46a08dcb14"},"cell_type":"code","source":"from sklearn.ensemble import BaggingClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.neighbors import KNeighborsClassifier\nmodel=BaggingClassifier(base_estimator=KNeighborsClassifier(n_neighbors=3),random_state=0,n_estimators=700)\nmodel.fit(X_train,Y_train)\ntrain_score=model.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"707fc91cd56a2f06fe9721b13b401967eaeaaacd"},"cell_type":"code","source":"from sklearn.ensemble import VotingClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\nfrom sklearn.naive_bayes import GaussianNB\nensemble_lin_rbf=VotingClassifier(estimators=[('KNN',KNeighborsClassifier(n_neighbors=10)),\n                                              ('RBF',svm.SVC(probability=True,kernel='rbf',C=0.5,gamma=0.1)),\n                                              ('RFor',RandomForestClassifier(n_estimators=500,random_state=0)),\n                                              ('LR',LogisticRegression(C=0.05)),\n                                              ('DT',DecisionTreeClassifier(random_state=0)),\n                                              ('NB',GaussianNB()),\n                                              ('svm',svm.SVC(kernel='linear',probability=True))\n                                             ], \n                       voting='soft')\nensemble_lin_rbf.fit(X_train,Y_train)\ntrain_score=ensemble_lin_rbf.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"4826481531560c666d2f7b2050f42534022a2000"},"cell_type":"code","source":"logis=linear_model.LogisticRegressionCV()\nlogis.fit(X_train,Y_train)\ntrain_score=logis.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"a62e095c9971928c419702b21cbfc7fb0b1b1baa"},"cell_type":"code","source":"from sklearn.model_selection import StratifiedKFold\nmodel1 = RandomForestClassifier()\nmodel1.fit(X_train,Y_train)\ntrain_score=model1.score(X_train,Y_train)\ntrain_score","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"9d68373c99de0a5d418acf414d4b0e6563b8ff4b"},"cell_type":"code","source":"y_test=model1.predict(data_test1)","execution_count":null,"outputs":[]},{"metadata":{"trusted":true,"_uuid":"d1533b5fe479649e0a332d14ecb0da1e75048670"},"cell_type":"code","source":"y_test","execution_count":null,"outputs":[]},{"metadata":{"_uuid":"4219f4740d0e7d21694fd21b813d6871d6f98436"},"cell_type":"markdown","source":"### Making Submission File"},{"metadata":{"trusted":true,"_uuid":"7dbf1462c5834aef30941e6ce96143319782f0a7"},"cell_type":"code","source":"submission = pd.DataFrame({\n        \"PassengerId\": data_test[\"PassengerId\"],\n        \"Survived\": y_test\n    })\nsubmission.to_csv('titanic_submission.csv', index=False)\n","execution_count":null,"outputs":[]}],"metadata":{"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"name":"python","version":"3.6.6","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat":4,"nbformat_minor":1}